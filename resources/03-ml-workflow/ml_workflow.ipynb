{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUuuaR0szzhX"
      },
      "source": [
        "# Very Basic Image Classifier in notebook\n",
        "\n",
        "This notebook contains a very basic image classifier to classify images of apples and oranges. This model is only for educational purposes and is not intended to be used in production. The model is trained on a very small dataset of 10 images of apples and 10 images of oranges, so the model is not expected to perform well on unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIPfzu2_Y56E"
      },
      "source": [
        "### Imports\n",
        "\n",
        "This code cell sets up the environment for working with different python packages. We import the following packages:\n",
        "\n",
        "- `tensorflow` for TensorFlow.\n",
        "- `ImageDataGenerator` from `tensorflow.keras.preprocessing.image` for image data augmentation.\n",
        "\n",
        "We check the TensorFlow version by print the TensorFlow version. This can be useful to verify that you're using the desired version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xNDr2DAXykfI",
        "outputId": "60e69392-24c7-46a0-e632-dff5dcbdd8b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.1\n"
          ]
        }
      ],
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exRo-qU_Y79W"
      },
      "source": [
        "### Constants\n",
        "\n",
        "This code cell defines a set of configuration constants that are commonly used when working with image datasets for machine learning tasks. These constants help streamline the process of setting up and training models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GpqlaOgOAgj2"
      },
      "outputs": [],
      "source": [
        "DATASET = \"dataset\"\n",
        "IMAGE_SIZE = (150, 150)\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3Ed1qIuzEEE"
      },
      "source": [
        "### Creating the dataset\n",
        "\n",
        "This shell command removes a directory and its contents using the rm command with the `-rf` flags. The `-r` flag stands for \"recursive\", which means that it will remove not only the directory itself but also all files and subdirectories within it. The `-f` flag stands for \"force\", which suppresses any confirmation prompts, making the removal process non-interactive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VemTPaf_1tXQ"
      },
      "outputs": [],
      "source": [
        "!rm -rf dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nPzYqzi23r-"
      },
      "source": [
        "This code cell is responsible for downloading image data and organizing it into a structured directory hierarchy. It is a common step in machine learning projects, especially when working with image classification tasks. The code downloads images of apples and oranges from various URLs and categorizes them into training, validation, and test sets.\n",
        "\n",
        "\n",
        "The dataset provided serves as a useful illustration of machine learning concepts, showcasing how to organize and prepare data for a model. However, it's important to note that this example dataset is exceptionally small in scale, containing just a handful of images of apples and oranges. In practice, real-world datasets can be significantly larger and more complex.\n",
        "\n",
        "Collecting real datasets for machine learning tasks often presents substantial challenges. Here are a few key considerations:\n",
        "\n",
        "- **Size:** Real datasets may consist of thousands or even millions of samples, necessitating extensive storage and computational resources for handling and processing.\n",
        "- **Labeling:** In many cases, each data point must be labeled or categorized correctly. Manual labeling can be a time-consuming and labor-intensive process, especially for large datasets.\n",
        "- **Diversity:** Real datasets often exhibit a wide range of variations, noise, and complexities, making them more representative of the challenges encountered in real-world applications.\n",
        "- **Bias and Fairness:** Ensuring that a dataset is unbiased and fairly represents diverse demographics and scenarios is crucial for ethical and accurate machine learning.\n",
        "- **Privacy and Compliance:** Handling sensitive or personal data requires strict adherence to privacy regulations, adding legal and ethical dimensions to dataset collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hi46fDqbPYA9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "for path in [\n",
        "    \"dataset/train/apples\",\n",
        "    \"dataset/val/apples\",\n",
        "    \"dataset/test/apples\",\n",
        "    \"dataset/train/oranges\",\n",
        "    \"dataset/val/oranges\",\n",
        "    \"dataset/test/oranges\",\n",
        "]:\n",
        "    os.makedirs(path)\n",
        "\n",
        "\n",
        "def download_from_list(list, type):\n",
        "    for i, img_name in enumerate(list):\n",
        "        response = requests.get(\n",
        "            f\"https://github.com/HOGENT-MLOps/mlops-labs/blob/main/resources/03-ml-workflow/img-lab3/{type}s/{img_name}?raw=true\"\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "\n",
        "        ml_split = \"train\"\n",
        "        if i == 9:\n",
        "            ml_split = \"test\"\n",
        "        elif i == 8:\n",
        "            ml_split = \"val\"\n",
        "\n",
        "        with open(f\"dataset/{ml_split}/{type}s/{type}{i}.jpeg\", \"wb\") as file:\n",
        "            file.write(response.content)\n",
        "\n",
        "\n",
        "download_from_list(\n",
        "    (f\"apple-{i}.jpeg\" for i in range(1, 11)),\n",
        "    \"apple\",\n",
        ")\n",
        "\n",
        "download_from_list(\n",
        "    (f\"orange-{i}.jpeg\" for i in range(1, 11)),\n",
        "    \"orange\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HM8Wl22zFZB"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "In the machine learning workflow, preprocessing is a crucial step that focuses on preparing and enhancing the raw data before it's fed into a model. It plays a pivotal role in shaping the success of a machine learning algorithm. Data preprocessing encompasses various tasks, including cleaning, transformation, and feature engineering.\n",
        "\n",
        "In the case of image data, as seen in the following code cell, preprocessing often includes resizing images, rescaling pixel values, and organizing data into batches. These steps ensure that the data is in a suitable format and distribution for training and evaluation, ultimately leading to more accurate and efficient machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Pr9pPD1gt8wQ",
        "outputId": "7c081527-73a9-4b0a-c5c6-cd88bf5a322a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 16 files belonging to 2 classes.\n",
            "Found 2 files belonging to 2 classes.\n",
            "Found 2 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from keras.utils import image_dataset_from_directory\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    directory=f\"{DATASET}/train\",\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='binary',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    directory=f\"{DATASET}/val\",\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='binary',\n",
        ")\n",
        "\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    directory=f\"{DATASET}/test\",\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='binary',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj3t53gizIXY"
      },
      "source": [
        "### Build the model\n",
        "\n",
        "This code cell defines and builds a Convolutional Neural Network (CNN) model using the Keras library. CNNs are a class of deep learning models commonly used for image classification and computer vision tasks. The model architecture is relatively simple, consisting of convolutional layers, activation functions, and dense layers.\n",
        "\n",
        "> **Challenge**: Can you think of changes we could make that could improve our final result?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6JQoWrVpDS9P",
        "outputId": "5de8bfdd-bbb3-4734-cff1-078cb44ecf14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Activation, Flatten, Dense\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(2))\n",
        "model.add(Activation('sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY__Cz_t4acZ"
      },
      "source": [
        "This code cell compiles a deep learning model using the Keras library. Compilation is an essential step in preparing the model for training. During compilation, you specify various aspects of the training process, such as the optimizer, loss function, and evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GIcfPoIgzKhn"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLNbqJkP4cPq"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "This code cell initiates the training of a deep learning model using the previously compiled model, training data generator, and validation data generator. It leverages the fit method in Keras to start the training process, and it also stores training history for later analysis and visualization.\n",
        "\n",
        "> **Note**: Training doesn't take long at all since we only have a toy dataset to work with. In real cases this can take hours or days to complete.\n",
        "\n",
        "> **Challenge**: Can you think of changes we could make that could speed up training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zkukoXcXzRoK",
        "outputId": "0ca14676-4199-4d51-be66-1f89c462dc02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py:635: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.3750 - loss: 13436.7002 - val_accuracy: 0.5000 - val_loss: 33537.5938\n",
            "Epoch 2/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 894ms/step - accuracy: 0.4583 - loss: 21553.7578 - val_accuracy: 0.5000 - val_loss: 2044.7690\n",
            "Epoch 3/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 878ms/step - accuracy: 0.3750 - loss: 2015.3828 - val_accuracy: 0.5000 - val_loss: 3370.2834\n",
            "Epoch 4/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 867ms/step - accuracy: 0.5417 - loss: 2911.2341 - val_accuracy: 0.5000 - val_loss: 1890.8081\n",
            "Epoch 5/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 1563.2719 - val_accuracy: 0.5000 - val_loss: 1601.5181\n",
            "Epoch 6/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 955ms/step - accuracy: 0.5417 - loss: 1109.5715 - val_accuracy: 0.5000 - val_loss: 2118.0771\n",
            "Epoch 7/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 890ms/step - accuracy: 0.5417 - loss: 1001.3564 - val_accuracy: 0.5000 - val_loss: 1097.7837\n",
            "Epoch 8/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 988ms/step - accuracy: 0.5000 - loss: 391.6215 - val_accuracy: 1.0000 - val_loss: 5.9605e-08\n",
            "Epoch 9/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.9583 - loss: 3.2448 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.6667 - loss: 191.6403 - val_accuracy: 1.0000 - val_loss: 1.1921e-07\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja_63YXyznV7"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "This code cell is responsible for evaluating the performance of a trained deep learning model on a test dataset. It uses the evaluate method in Keras to compute the model's test loss and accuracy based on the test data generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KrROu9JyzTjt",
        "outputId": "df4f36d3-cb24-47ba-bec5-6ee0d7090ac5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Test accuracy: 1.0\n",
            "Test Loss: 0.0\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_accuracy = model.evaluate(\n",
        "    test_dataset,\n",
        "    steps=len(test_dataset)\n",
        ")\n",
        "\n",
        "print('Test accuracy:', test_accuracy)\n",
        "print('Test Loss:', test_loss)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}